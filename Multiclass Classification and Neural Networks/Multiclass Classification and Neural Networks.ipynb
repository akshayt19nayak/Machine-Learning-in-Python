{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to solve the third programming assignment of Andrew Ng's Machine Learning course on Coursera using Python. I have also taken a snapshot of the important concepts relating to the topics taught in this week that I'll be pasting in my notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Multiclass Classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "from scipy.io import loadmat\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " '__globals__': [],\n",
       " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ..., \n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To read the matlab matrix in Python\n",
    "data = loadmat('ex3data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the matrices X and Y are (5000, 400) and (5000, 1) respectively.\n",
      "The number of unique values in y are: [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "X=data['X']\n",
    "y=data['y']\n",
    "print('The dimension of the matrices X and Y are',X.shape,'and',y.shape,'respectively.')\n",
    "print('The number of unique values in y are:',np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row in the matrix is an image. The dimension of every image is 20x20 and these 400 pixels have been unrolled into one vector, thus the dimension of the matrix is 5000x400. y has 5000 labels - the class to which every picture belongs. Now, pictures with 0 have the class label 10. This has been done for the purpose of this course as indexing in MATLAB starts from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We'll change this in python\n",
    "y=np.where(y==10,0,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label is 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUOKEUOKEUP8CmfdXoDO8qmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x540b08f6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To show the X and y for one particular image\n",
    "plt.imshow(X[0].reshape(20,20),cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "print('The label is',y[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To add an additional column. Why is it needed? Since we're trying to perform matrix multiplications, it is \n",
    "#necessary that the conditions for matrix multiplication are satisfied. Since theta 0 doesn't have a \n",
    "#corresponding column in X, to be consistent we need to have an additional column that only has ones\n",
    "X=np.hstack((np.ones((X.shape[0],1)),X))\n",
    "theta=np.zeros(X.shape[1]) #Initial value of theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SIGMOID(z):\n",
    "    #Using np.exp is beneficial as it does the operation in a vectorised manner\n",
    "    return(1/(1+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def COSTREG(theta,X,y,lam):\n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    theta=theta.reshape(n,1)\n",
    "    hypothesis=SIGMOID(np.dot(X,theta))\n",
    "    J=(-1/m)*((np.dot(np.transpose(y),np.log(hypothesis)))+\n",
    "              (np.dot((1-np.transpose(y)),np.log(1-hypothesis))))+(lam/(2*m))*np.sum(np.square(theta[1:]))\n",
    "    return(J.flatten()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- The COSTREG function calculates the regularized cost of our logistic regression classifier with respect to the thetas (parameters)\n",
    "- m is the number of rows in the design matrix (X)\n",
    "- n is the number of features in the original X plus 1\n",
    "- we first take a dot product of our X and theta (initial values set to 0). X is an (m x n) dimensional matrix, theta is a n dimensional vector. The result of the dot product gives us an m dimensional vector (our hypothesis)\n",
    "- y is the label which is an m dimesnional vector \n",
    "- We then take the transpose of y which becomes a (1 x m) matrix and then take the dot product of this transpose with the log of the hypothesis (m x 1) matrix\n",
    "- The result gives us our cost for y=1. We do the exact same operation for cost when y=0 as well and add the two terms\n",
    "- However in this case, we also have our regularization parameter lambda. We don't regularize the zeroth theta which is our intercept term. We square and sum over all other terms\n",
    "- The result of these operations gives us our final cost which we try to minimze with respect to theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GRADREG(theta,X,y,lam):\n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    theta=theta.reshape(n,1)\n",
    "    hypothesis=SIGMOID(np.dot(X,theta))\n",
    "    grad=(1/m)*(np.dot(np.transpose(X),(hypothesis-y)))+np.insert((lam/m)*(theta[1:]),0,0).reshape(n,1)\n",
    "    return(grad.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- The GRADREG function calculates the gradient of the regularized cost function\n",
    "- The gradient looks similar to the gradient of the linear regression cost function, however the key difference in the hypothesis\n",
    "- We take a transpose of X, so that we get a (n x m) dimensional matrix\n",
    "- (hypotheis - y) is an (m x 1) dimensional matrix\n",
    "- The result of the dot product of the two gives us an n dimensional vector, that comes in handy in updating the value of theta.\n",
    "- However in this case, we also have our regularization parameter lambda. We don't regularize the zeroth theta which is our intercept term\n",
    "- We multiply all our thetas starting from index 1 by lambda/m\n",
    "- Since the zeroth term in theta is exempt from regularization, we need to set it to zero in the second half of the equation to be consistent with the dimensions for which we use the np.insert function. DON'T SET IT TO theta[0] as this will mean that after every iteration of an optimization algorithm, theta[0] at the end of the previous iteration will get added to theta[0] of the new iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the scipy's optimize module, we make use of Truncated Newton Method to optimize our thetas. Note : It is\n",
    "#important that the parameters passed in the COST and GRAD function are in the order - theta, X and y. You could\n",
    "#spend hours debugging the error if the parameters are not in that order\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onevsall(X,y,k,theta,lam):\n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    thetas=[]\n",
    "    for i in range(k):\n",
    "        final_theta=opt.fmin_tnc(func=COSTREG, x0=theta, fprime=GRADREG, args=(X,np.where(y==i,1,0),lam))[0]\n",
    "        thetas.append(final_theta)\n",
    "    thetas=np.array(thetas).T\n",
    "    return(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thetas=onevsall(X,y,10,theta,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- In multiclass classification, we train k classifiers for k classes\n",
    "- We run a for loop, looping from the 0th to the kth class\n",
    "- We store the value of y as a boolean array, where in the ith iteration we only have two unique values in y - 0 and i\n",
    "- On every iteration, we basically lump all of our training data in these two classes - 0 and i and find the optimal value of theta (final_theta) that minimizes the cost for the ith logistic regression classifier\n",
    "- These thetas are then appended to a list (thetas) and converted to a numpy array\n",
    "- The thetas have the dimension (k x n), where we have the parameters for k logistic regression classifiers on every row\n",
    "- We return the transpose of this matrix which has the dimensions (n x k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_multiclass(thetas,X):\n",
    "    predictions=SIGMOID(np.dot(X,thetas))\n",
    "    final_pred=np.argmax(predictions,axis=1)\n",
    "    return(final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- Once we get the thetas matrix, we mutliply (take a dot product) of X which as a dimension of (5000 x 401) and thetas (401 x 10)\n",
    "- We then store the result in a matrix called final_pred (5000 x 10) \n",
    "- Our job here is to pick the classifier that has the maximum value of hypothesis \n",
    "- If we had 3 classes and the hypotheses of the three classifiers were [0.3 0.9 0.5], we would pick the second classifier's prediction here as it seems to be confident about it's prediction. Hence the predicted class label in this case would be 1 (since 0.9's index is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is 96.46 %\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the classifier is',np.mean(pred_multiclass(thetas,X)==y.flatten())*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could add more features (such as polynomial features) to logistic regression, but that can be very expensive to train. Hence we use neural networks in this part of the exercise. Neural networks offers an alternate way to perform machine learning when we have complex hypotheses with many features. Adding intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN Model Representation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN Model Representation 2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer gets its own matrix of weights, Θ(j).\n",
    "\n",
    "The dimensions of these matrices of weights is determined as follows:\n",
    "\n",
    "- If network has s(j) units in layer j and s(j+1) units in layer j+1, then Θ(j) will be of dimension s(j+1)×(s(j)+1) \n",
    "- The +1 comes from the addition in Θ(j) of the \"bias nodes,\" x0. In other words the output nodes will not include the bias nodes while the inputs will"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN Model Representation 3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the matrices X and Y are (5000, 400) and (5000, 1) respectively.\n",
      "The number of unique values in y are: [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "X=data['X']\n",
    "y=data['y']\n",
    "print('The dimension of the matrices X and Y are',X.shape,'and',y.shape,'respectively.')\n",
    "print('The number of unique values in y are:',np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To load the MATLAB file onto Python\n",
    "weights = loadmat('ex3weights.mat')\n",
    "Theta1=weights['Theta1']\n",
    "Theta2=weights['Theta2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of Theta1 and Theta2 are (25, 401) and (10, 26) respectively.\n"
     ]
    }
   ],
   "source": [
    "print('The dimensions of Theta1 and Theta2 are',Theta1.shape,'and',Theta2.shape,'respectively.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN Model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_nn(X,Theta1,Theta2):\n",
    "    m=X.shape[0]\n",
    "    a1=np.hstack((np.ones((m,1)),X))\n",
    "    z2=np.dot(a1,Theta1.T)\n",
    "    a2=SIGMOID(z2)\n",
    "    a2=np.hstack((np.ones((m,1)),a2))\n",
    "    z3=np.dot(a2,Theta2.T)\n",
    "    a3=SIGMOID(z3)\n",
    "    predictions=np.argmax(a3,axis=1)+1\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- a1(X) is a matrix of dimension (m x n+1)\n",
    "- z2 is a dot product of a1 and transpose of Theta1\n",
    "- a2 is the activation in the second layer and is = g(z2)\n",
    "- We then add a column of 1s that represents the bias node in the second layer\n",
    "- z3 is a dot product of a2 and transpose of Theta2\n",
    "- a3 is the final hypothesis and = g(z3)\n",
    "- Like in the multiclass classification problem we choose the classifier that has the maximum value for the hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is 97.52 %\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the classifier is',np.mean(predict_nn(X,Theta1,Theta2)==y.flatten())*100,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
