{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to solve the third programming assignment of Andrew Ng's Machine Learning course on Coursera using Python. I have also taken a snapshot of the important concepts relating to the topics taught in this week that I'll be pasting in my notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "from scipy.io import loadmat\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " '__globals__': [],\n",
       " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ..., \n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To read the matlab matrix in Python\n",
    "data = loadmat('ex4data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the matrices X and Y are (5000, 400) and (5000, 1) respectively.\n",
      "The number of unique values in y are: [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "X=data['X']\n",
    "y=data['y']\n",
    "print('The dimension of the matrices X and Y are',X.shape,'and',y.shape,'respectively.')\n",
    "print('The number of unique values in y are:',np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row in the matrix is an image. The dimension of every image is 20x20 and these 400 pixels have been unrolled into one vector, thus the dimension of the matrix is 5000x400. y has 5000 labels - the class to which every picture belongs. Now, pictures with 0 have the class label 10. This has been done for the purpose of this course as indexing in MATLAB starts from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label is 10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUOKEUOKEUP8CmfdXoDO8qmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc2b2ddec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To show the X and y for one particular image\n",
    "plt.imshow(X[0].reshape(20,20),cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "print('The label is',y[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of Theta1 and Theta2 are (25, 401) and (10, 26) respectively.\n"
     ]
    }
   ],
   "source": [
    "#To load the weights from the MATLAB file onto Python\n",
    "weights = loadmat('ex4weights.mat')\n",
    "Theta1=weights['Theta1']\n",
    "Theta2=weights['Theta2']\n",
    "print('The dimensions of Theta1 and Theta2 are',Theta1.shape,'and',Theta2.shape,'respectively.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN Cost Function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part is like a double for loop, where we are iterating through each of our 'm' training examples and for every training example we are outputting a hypothesis for each of the 'K' classes. \n",
    "\n",
    "The second part is a triple sum where we are essentially trying to regularize the thetas in our entire network. Corresponding to 'L' layers of the neural network, we will be having (L-1) thetas, and corresponding to each theta, we will iterate through each of the columns and within each column we will be squaring all the thetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SIGMOID(z):\n",
    "    #Using np.exp is beneficial as it does the operation in a vectorised manner\n",
    "    return(1/(1+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Neural Networks, the gradient of the thetas is a matrix and not a vector like in the previous exercises. However our optimization algorithms take in thetas as vectors and not matrices. Thus we pass thetas as a vector and later reshape it inside the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Theta=np.array(Theta1.flatten().tolist()+Theta2.flatten().tolist())\n",
    "#Unregularised cost function\n",
    "def NNCOST(Theta,X,y,K,hidden_layer_size):\n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    \n",
    "    Theta1=Theta[:hidden_layer_size*(n+1)].reshape(hidden_layer_size,n+1)\n",
    "    Theta2=Theta[hidden_layer_size*(n+1):].reshape(K,hidden_layer_size+1)\n",
    "    \n",
    "    a1=np.hstack((np.ones((m,1)),X))\n",
    "    z2=np.dot(a1,Theta1.T)\n",
    "    a2=SIGMOID(z2)\n",
    "    a2=np.hstack((np.ones((m,1)),a2))\n",
    "    z3=np.dot(a2,Theta2.T)\n",
    "    a3=SIGMOID(z3)\n",
    "    yk=np.zeros((m,K))\n",
    "    for i in range(m):\n",
    "        yk[i,y[i]-1]=1\n",
    "    cost=(-1/m)*sum(sum(np.multiply(yk,np.log(a3))+np.multiply(1-yk,np.log(1-a3))))\n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- a1(X) is a matrix of dimension (m x n+1)\n",
    "- z2 is a dot product of a1 and transpose of Theta1\n",
    "- a2 is the activation in the second layer and is = g(z2)\n",
    "- We then add a column of 1s that represents the bias node in the second layer\n",
    "- z3 is a dot product of a2 and transpose of Theta2\n",
    "- a3 is the final hypothesis and = g(z3)\n",
    "- yk is a (m x K) dimensional matrix that has 1s at the positions defined by the class that the ith value in y belongs to\n",
    "- We then do an element wise multiplication of yk with the (m x K) dimensional hypothesis and then sum over the costs of misclassification. This resembles our cost function for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28762916516131881"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNCOST(Theta,X,y,10,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regularised cost function\n",
    "def NNCOSTREG(Theta,X,y,K,hidden_layer_size,lam):\n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    \n",
    "    Theta1=Theta[:hidden_layer_size*(n+1)].reshape(hidden_layer_size,n+1)\n",
    "    Theta2=Theta[hidden_layer_size*(n+1):].reshape(K,hidden_layer_size+1)\n",
    "    \n",
    "    a1=np.hstack((np.ones((m,1)),X))\n",
    "    z2=np.dot(a1,Theta1.T)\n",
    "    a2=SIGMOID(z2)\n",
    "    a2=np.hstack((np.ones((m,1)),a2))\n",
    "    z3=np.dot(a2,Theta2.T)\n",
    "    a3=SIGMOID(z3)\n",
    "    yk=np.zeros((m,K))\n",
    "    for i in range(m):\n",
    "        yk[i,y[i]-1]=1\n",
    "    cost=(-1/m)*sum(sum(np.multiply(yk,np.log(a3))+np.multiply(1-yk,np.log(1-a3))))\n",
    "    reg=(lam/(2*m))*(sum(sum(np.square(Theta1[:,1:])))+sum(sum(np.square(Theta2[:,1:]))))\n",
    "    return(cost+reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- a1(X) is a matrix of dimension (m x n+1)\n",
    "- z2 is a dot product of a1 and transpose of Theta1\n",
    "- a2 is the activation in the second layer and is = g(z2)\n",
    "- We then add a column of 1s that represents the bias node in the second layer\n",
    "- z3 is a dot product of a2 and transpose of Theta2\n",
    "- a3 is the final hypothesis and = g(z3)\n",
    "- yk is a (m x K) dimensional matrix that has 1s at the positions defined by the class that the ith value in y belongs to\n",
    "- We then do an element wise multiplication of yk with the (m x K) dimensional hypothesis and then sum over the costs of misclassification. This resembles our cost function for logistic regression\n",
    "- However we now have to regularize our thetas, by squaring over all terms except the first column and multiplying with (lambda/2m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38376985909092354"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNCOSTREG(Theta,X,y,10,25,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN Backpropogation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SIGMOIDGRAD(z):\n",
    "    g=(1/(1+np.exp(-z)))\n",
    "    return(np.multiply((g),(1-g)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- This is the gradient of the SIGMOID function\n",
    "\n",
    "![title](NN Sigmoid Grad 1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN Rand Init.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RANDINIT(L_in,L_out):\n",
    "    eps_init=np.sqrt(6)/np.sqrt(L_in+L_out)\n",
    "    return(np.random.rand(L_in,L_out)*2*eps_init - eps_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BACKPROP(Theta,X,y,K,hidden_layer_size):\n",
    "    \n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    \n",
    "    Theta1=Theta[:hidden_layer_size*(n+1)].reshape(hidden_layer_size,n+1)\n",
    "    Theta2=Theta[hidden_layer_size*(n+1):].reshape(K,hidden_layer_size+1)\n",
    "    \n",
    "    yk=np.zeros((m,K))\n",
    "    for i in range(m):\n",
    "        yk[i,y[i]-1]=1\n",
    "    \n",
    "    DELTA_1=0\n",
    "    DELTA_2=0\n",
    "    \n",
    "    for i in range(m):\n",
    "        a1=np.hstack((1,X[i,:])).reshape(1,n+1) #Stacking to a one dimensional array\n",
    "        z2=np.dot(a1,Theta1.T) \n",
    "        a2=SIGMOID(z2)\n",
    "        a2=np.hstack((np.ones((1,1)),a2)).reshape(1,hidden_layer_size+1) #Stacking to a two dimensional array\n",
    "        z3=np.dot(a2,Theta2.T)\n",
    "        a3=SIGMOID(z3).reshape(1,K) \n",
    "        \n",
    "        yi=yk[i,:].reshape(1,K) #1x(no. of classes)\n",
    "        \n",
    "        delta_3=(a3-yi).reshape(1,K) #1x(no. of classes)\n",
    "        delta_2=np.multiply(np.dot(delta_3,Theta2[:,1:]),\n",
    "                            SIGMOIDGRAD(z2)).reshape(1,hidden_layer_size) #1x(no. of nodes in the middle layer)\n",
    "        \n",
    "        DELTA_1=DELTA_1+np.dot(delta_2.T,a1) #(no. of nodes in the middle layer)x(n+1)\n",
    "        DELTA_2=DELTA_2+np.dot(delta_3.T,a2) #(no. of classes)x(no. of nodes in the middle layer + 1)\n",
    "    \n",
    "    Theta1_grad=(1/m)*DELTA_1 #same dimensions as Theta1\n",
    "    Theta2_grad=(1/m)*DELTA_2 #same dimensions as Theta2\n",
    "    \n",
    "    Theta_grad=np.array(Theta1_grad.flatten().tolist()+Theta2_grad.flatten().tolist())\n",
    "    \n",
    "    return(Theta_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "\n",
    "- Backpropogation as an algorithm, has a similar objective to that of gradient descent where we are essentially trying to minimize the cost function    \n",
    "- We set DELTA_1 and DELTA_2 to 0 as we treat it as an accumalator to add up our values as we go along and eventually compute the partial derivative of the J with respect to the Theta matrices \n",
    "- Inside the for loop, we iterate through every row of the design matrix X and compute the activations in the second and the third layer (a2 and a3)\n",
    "- When we calculate the deltas, we are essentially trying to find the error of node j in layer L. delta_3 is the error term for the the 10 nodes in the last layer\n",
    "- We then again calculate the deltas in the second layer (delta_2), which is found by taking a dot product of delta_3 and Theta2 (excluding the first column as the first column is for the bias unit). We term this method as backpropogation as were are trying to take the errors of the (n+1)th layer to the nth layer and then try to change the set of thetas that map the activation units from the nth to the (n+1)th layer. We also make an element wise mutiplication with the the sigmoid gradient of z2 (z2 as we are ignoring the bias unit in a2)\n",
    "- The DELTA_1 and DELTA_2 values get updated after every iteration (that is from the 1st to mth value in X)\n",
    "- From the perspective of the neural network, we only really complete one iteration and what we have essentially calculated is the partial derivative of J with respect to the Theta matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To randomly initialize the Thetas so as to break symmetry\n",
    "initial_theta1=RANDINIT(25,401)\n",
    "initial_theta2=RANDINIT(10,26)\n",
    "initial_theta=np.array(initial_theta1.flatten().tolist()+initial_theta2.flatten().tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.67400565545932756\n",
       "     jac: array([ -3.44450424e-04,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "        -2.36636020e-05,  -7.81020380e-03,  -8.02578183e-03])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 50\n",
       "     nit: 8\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([ 0.51913547, -0.0248553 ,  0.03239696, ..., -1.25231209,\n",
       "       -0.81829371,  1.14697397])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_result_50=opt.minimize(fun=NNCOST, x0=initial_theta, jac=BACKPROP, method='TNC',\n",
    "                args=(X,y,10,25), options={'maxiter': 50})\n",
    "opt_result_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.036467775635895879\n",
       "     jac: array([  5.43305092e-06,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         5.81957235e-05,   7.33445911e-05,   1.66545889e-04])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 250\n",
       "     nit: 19\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([ 1.29262489, -0.0248553 ,  0.03239696, ..., -1.41841189,\n",
       "       -1.44696555,  1.56271153])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increasing the number of iterations\n",
    "opt_result_250=opt.minimize(fun=NNCOST, x0=initial_theta, jac=BACKPROP, method='TNC',\n",
    "                args=(X,y,10,25), options={'maxiter': 250})\n",
    "opt_result_250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so the function value has decreased a lot with an increase in the number of iterations, which means it is possible that our neural network has overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is 99.9 %\n"
     ]
    }
   ],
   "source": [
    "#To compute the accuracy \n",
    "final_theta=opt_result_250['x']\n",
    "def predict_nn(Theta,X,K,hidden_layer_size):\n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    \n",
    "    Theta1=Theta[:hidden_layer_size*(n+1)].reshape(hidden_layer_size,n+1)\n",
    "    Theta2=Theta[hidden_layer_size*(n+1):].reshape(K,hidden_layer_size+1)\n",
    "    \n",
    "    a1=np.hstack((np.ones((m,1)),X))\n",
    "    z2=np.dot(a1,Theta1.T)\n",
    "    a2=SIGMOID(z2)\n",
    "    a2=np.hstack((np.ones((m,1)),a2))\n",
    "    z3=np.dot(a2,Theta2.T)\n",
    "    a3=SIGMOID(z3)\n",
    "    predictions=np.argmax(a3,axis=1)+1\n",
    "    return(predictions)\n",
    "\n",
    "print('The accuracy of the classifier is',np.mean(predict_nn(final_theta,X,10,25)==y.flatten())*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "\n",
    "- a1(X) is a matrix of dimension (m x n+1)\n",
    "- z2 is a dot product of a1 and transpose of Theta1\n",
    "- a2 is the activation in the second layer and is = g(z2)\n",
    "- We then add a column of 1s that represents the bias node in the second layer\n",
    "- z3 is a dot product of a2 and transpose of Theta2\n",
    "- a3 is the final hypothesis and = g(z3)\n",
    "- yk is a (m x K) dimensional matrix that has 1s at the positions defined by the class that the ith value in y belongs to\n",
    "- Like in the multiclass classification problem we choose the hypothesis that has the maximum value for a given class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now although this accuracy is stellar, we don't know if the model will generalize well to unseen tests cases. Thus we need to regularize the thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BACKPROPREG(Theta,X,y,K,hidden_layer_size,lam):\n",
    "    \n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    \n",
    "    Theta1=Theta[:hidden_layer_size*(n+1)].reshape(hidden_layer_size,n+1)\n",
    "    Theta2=Theta[hidden_layer_size*(n+1):].reshape(K,hidden_layer_size+1)\n",
    "    \n",
    "    yk=np.zeros((m,K))\n",
    "    for i in range(m):\n",
    "        yk[i,y[i]-1]=1\n",
    "    \n",
    "    DELTA_1=0\n",
    "    DELTA_2=0\n",
    "    \n",
    "    for i in range(m):\n",
    "        a1=np.hstack((1,X[i,:])).reshape(1,n+1) #Stacking to a one dimensional array\n",
    "        z2=np.dot(a1,Theta1.T) \n",
    "        a2=SIGMOID(z2)\n",
    "        a2=np.hstack((np.ones((1,1)),a2)).reshape(1,hidden_layer_size+1) #Stacking to a two dimensional array\n",
    "        z3=np.dot(a2,Theta2.T)\n",
    "        a3=SIGMOID(z3).reshape(1,K) \n",
    "        \n",
    "        yi=yk[i,:].reshape(1,K) #1x(no. of classes)\n",
    "        \n",
    "        delta_3=(a3-yi).reshape(1,K) #1x(no. of classes)\n",
    "        delta_2=np.multiply(np.dot(delta_3,Theta2[:,1:]),\n",
    "                            SIGMOIDGRAD(z2)).reshape(1,hidden_layer_size) #1x(no. of nodes in the middle layer)\n",
    "        \n",
    "        DELTA_1=DELTA_1+np.dot(delta_2.T,a1) #(no. of nodes in the middle layer)x(n+1)\n",
    "        DELTA_2=DELTA_2+np.dot(delta_3.T,a2) #(no. of classes)x(no. of nodes in the middle layer + 1)\n",
    "    \n",
    "    Theta1_grad=(1/m)*DELTA_1 + np.hstack((np.zeros((hidden_layer_size,1)),(lam/m)*DELTA_1[:,1:]))\n",
    "    Theta2_grad=(1/m)*DELTA_2 + np.hstack((np.zeros((K,1)),(lam/m)*DELTA_2[:,1:]))\n",
    "    \n",
    "    Theta_grad=np.array(Theta1_grad.flatten().tolist()+Theta2_grad.flatten().tolist())\n",
    "    \n",
    "    return(Theta_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "\n",
    "- Backpropogation as an algorithm, has a similar objective to that of gradient descent where we are essentially trying to minimize the cost function    \n",
    "- We set DELTA_1 and DELTA_2 to 0 as we treat it as an accumalator to add up our values as we go along and eventually compute the partial derivative of the J with respect to the Theta matrices \n",
    "- Inside the for loop, we iterate through every row of the design matrix X and compute the activations in the second and the third layer (a2 and a3)\n",
    "- When we calculate the deltas, we are essentially trying to find the error of node j in layer L. delta_3 is the error term for the the 10 nodes in the last layer\n",
    "- We then again calculate the deltas in the second layer (delta_2), which is found by taking a dot product of delta_3 and Theta2 (excluding the first column as the first column is for the bias unit). We term this method as backpropogation as were are trying to take the errors of the (n+1)th layer to the nth layer and then try to change the set of thetas that map the activation units from the nth to the (n+1)th layer. We also make an element wise mutiplication with the the sigmoid gradient of z2 (z2 as we are ignoring the bias unit in a2)\n",
    "- The DELTA_1 and DELTA_2 values get updated after every iteration (that is from the 1st to mth value in X)\n",
    "- Then we regularize the columns in the Theta matrices excluding the first column\n",
    "- From the perspective of the neural network, we only really complete one iteration and what we have essentially calculated is the partial derivative of J with respect to the Theta matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.92933629469925805\n",
       "     jac: array([ 0.00024908,  0.        ,  0.        , ...,  0.00149312,\n",
       "       -0.00184368, -0.00205029])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 50\n",
       "     nit: 6\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([ 0.10195274, -0.0248553 ,  0.03239696, ..., -0.93383299,\n",
       "       -0.78851334,  1.22254729])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_result_50_REG=opt.minimize(fun=NNCOSTREG, x0=initial_theta, jac=BACKPROPREG, method='TNC',\n",
    "                args=(X,y,10,25,1), options={'maxiter': 50})\n",
    "opt_result_50_REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.58789165644948604\n",
       "     jac: array([-0.00092871,  0.        ,  0.        , ...,  0.00018626,\n",
       "       -0.0017618 , -0.00316913])\n",
       " message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n",
       "    nfev: 240\n",
       "     nit: 15\n",
       "  status: 1\n",
       " success: True\n",
       "       x: array([ 0.50625545, -0.0248553 ,  0.03239696, ..., -1.38966719,\n",
       "       -1.16734853,  1.95659102])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increasing the number of iterations\n",
    "opt_result_250_REG=opt.minimize(fun=NNCOSTREG, x0=initial_theta, jac=BACKPROPREG, method='TNC',\n",
    "                args=(X,y,10,25,1), options={'maxiter': 250})\n",
    "opt_result_250_REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is 97.48 %\n"
     ]
    }
   ],
   "source": [
    "final_theta_REG=opt_result_250_REG['x']\n",
    "print('The accuracy of the classifier is',np.mean(predict_nn(final_theta_REG,X,10,25)==y.flatten())*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is pretty impressive considering we have incorporated regularization in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation Intuition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN Backpropogation Intuition.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
